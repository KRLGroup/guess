{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import vision\n",
    "\n",
    "def normalize(data, label):\n",
    "    return data.astype(np.float32, copy=False)/(255.0/2) - 1.0, label.astype(np.float32)\n",
    "\n",
    "preprocessing = vision.transforms.Compose([vision.transforms.Resize(size=(64, 64)),\n",
    "                                           normalize])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "mnist_train = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=preprocessing),\n",
    "                                      batch_size, shuffle=True)\n",
    "mnist_test = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=preprocessing),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The downloaded data is of type `Dataset` which are\n",
    "# Well suited to work with the new Gluon interface but less\n",
    "# With the older symbol API, used in this tutorial.\n",
    "# Therefore we convert them to numpy array first\n",
    "# X = np.zeros((70000, 28, 28))\n",
    "# for i, (data, label) in enumerate(mnist_train):\n",
    "#     X[i] = data.asnumpy()[:,:,0]\n",
    "# for i, (data, label) in enumerate(mnist_test):\n",
    "#     X[len(mnist_train)+i] = data.asnumpy()[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a seed so that we get the same random permutation each time\n",
    "# np.random.seed(1)\n",
    "# p = np.random.permutation(X.shape[0])\n",
    "# X = X[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# X = np.asarray([cv2.resize(x, (64,64)) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.astype(np.float32, copy=False)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.reshape((70000, 1, 64, 64))\n",
    "# X = np.tile(X, (1, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mxnet as mx\n",
    "# batch_size = 64\n",
    "# image_iter = mx.io.NDArrayIter(X, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandIter(mx.io.DataIter):\n",
    "    def __init__(self, batch_size, ndim):\n",
    "        self.batch_size = batch_size\n",
    "        self.ndim = ndim\n",
    "        self.provide_data = [('rand', (batch_size, ndim, 1, 1))]\n",
    "        self.provide_label = []\n",
    "\n",
    "    def iter_next(self):\n",
    "        return True\n",
    "\n",
    "    def getdata(self):\n",
    "        #Returns random numbers from a gaussian (normal) distribution\n",
    "        #with mean=0 and standard deviation = 1\n",
    "        return [mx.random.normal(0, 1.0, shape=(self.batch_size, self.ndim, 1, 1))]\n",
    "Z = 100\n",
    "rand_iter = RandIter(batch_size, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fc = 512\n",
    "bias = True\n",
    "bepsilon = 1e-5 + 1e-12\n",
    "\n",
    "netG = gluon.nn.Sequential()\n",
    "with netG.name_scope():\n",
    "    netG.add(gluon.nn.Conv2DTranspose(1024, (4, 4), strides=(2,2), padding=(1,1), use_bias=bias))\n",
    "    netG.add(gluon.nn.BatchNorm(epsilon=bepsilon))\n",
    "    netG.add(gluon.nn.Activation('relu'))\n",
    "    \n",
    "    netG.add(gluon.nn.Conv2DTranspose(512, (4, 4), strides=(2,2), padding=(1,1), use_bias=bias))\n",
    "    netG.add(gluon.nn.BatchNorm(epsilon=bepsilon))\n",
    "    netG.add(gluon.nn.Activation('relu'))\n",
    "    \n",
    "    netG.add(gluon.nn.Conv2DTranspose(256, (4, 4), strides=(2,2), padding=(1,1), use_bias=bias))\n",
    "    netG.add(gluon.nn.BatchNorm(epsilon=bepsilon))\n",
    "    netG.add(gluon.nn.Activation('relu'))\n",
    "\n",
    "    netG.add(gluon.nn.Conv2DTranspose(128, (4, 4), strides=(2,2), padding=(1,1), use_bias=bias))\n",
    "    netG.add(gluon.nn.BatchNorm(epsilon=bepsilon))\n",
    "    netG.add(gluon.nn.Activation('relu'))\n",
    "    \n",
    "    netG.add(gluon.nn.Conv2DTranspose(3, (4, 4), strides=(2,2), padding=(1,1), use_bias=bias))\n",
    "    netG.add(gluon.nn.Activation('tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generatorSymbol = mx.sym.Activation(g5, name='gact5', act_type='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "\n",
    "d1 = mx.sym.Convolution(data, name='d1', kernel=(4,4), stride=(2,2), pad=(1,1), num_filter=128, no_bias=no_bias)\n",
    "dact1 = mx.sym.LeakyReLU(d1, name='dact1', act_type='leaky', slope=0.2)\n",
    "\n",
    "d2 = mx.sym.Convolution(dact1, name='d2', kernel=(4,4), stride=(2,2), pad=(1,1), num_filter=256, no_bias=no_bias)\n",
    "dbn2 = mx.sym.BatchNorm(d2, name='dbn2', fix_gamma=fix_gamma, eps=epsilon)\n",
    "dact2 = mx.sym.LeakyReLU(dbn2, name='dact2', act_type='leaky', slope=0.2)\n",
    "\n",
    "d3 = mx.sym.Convolution(dact2, name='d3', kernel=(4,4), stride=(2,2), pad=(1,1), num_filter=512, no_bias=no_bias)\n",
    "dbn3 = mx.sym.BatchNorm(d3, name='dbn3', fix_gamma=fix_gamma, eps=epsilon)\n",
    "dact3 = mx.sym.LeakyReLU(dbn3, name='dact3', act_type='leaky', slope=0.2)\n",
    "\n",
    "d4 = mx.sym.Convolution(dact3, name='d4', kernel=(4,4), stride=(2,2), pad=(1,1), num_filter=1024, no_bias=no_bias)\n",
    "dbn4 = mx.sym.BatchNorm(d4, name='dbn4', fix_gamma=fix_gamma, eps=epsilon)\n",
    "dact4 = mx.sym.LeakyReLU(dbn4, name='dact4', act_type='leaky', slope=0.2)\n",
    "\n",
    "d5 = mx.sym.Convolution(dact4, name='d5', kernel=(4,4), num_filter=1, no_bias=no_bias)\n",
    "d5 = mx.sym.Flatten(d5)\n",
    "\n",
    "label = mx.sym.Variable('label')\n",
    "discriminatorSymbol = mx.sym.LogisticRegressionOutput(data=d5, label=label, name='dloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "sigma = 0.02\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "# Define the compute context, use GPU if available\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "#=============Generator Module=============\n",
    "generator = mx.mod.Module(symbol=generatorSymbol, data_names=('rand',), label_names=None, context=ctx)\n",
    "generator.bind(data_shapes=rand_iter.provide_data)\n",
    "generator.init_params(initializer=mx.init.Normal(sigma))\n",
    "generator.init_optimizer(\n",
    "    optimizer='adam',\n",
    "    optimizer_params={\n",
    "        'learning_rate': lr,\n",
    "        'beta1': beta1,\n",
    "    })\n",
    "mods = [generator]\n",
    "\n",
    "# =============Discriminator Module=============\n",
    "discriminator = mx.mod.Module(symbol=discriminatorSymbol, data_names=('data',), label_names=('label',), context=ctx)\n",
    "discriminator.bind(data_shapes=image_iter.provide_data,\n",
    "          label_shapes=[('label', (batch_size,))],\n",
    "          inputs_need_grad=True)\n",
    "discriminator.init_params(initializer=mx.init.Normal(sigma))\n",
    "discriminator.init_optimizer(\n",
    "    optimizer='adam',\n",
    "    optimizer_params={\n",
    "        'learning_rate': lr,\n",
    "        'beta1': beta1,\n",
    "    })\n",
    "mods.append(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Takes the images in the batch and arranges them in an array so that they can be\n",
    "#Plotted using matplotlib\n",
    "def fill_buf(buf, num_images, img, shape):\n",
    "    width = buf.shape[0]/shape[1]\n",
    "    height = buf.shape[1]/shape[0]\n",
    "    img_width = int(num_images%width)*shape[0]\n",
    "    img_hight = int(num_images/height)*shape[1]\n",
    "    buf[img_hight:img_hight+shape[1], img_width:img_width+shape[0], :] = img\n",
    "\n",
    "#Plots two images side by side using matplotlib\n",
    "def visualize(fake, real):\n",
    "    #64x3x64x64 to 64x64x64x3\n",
    "    fake = fake.transpose((0, 2, 3, 1))\n",
    "    #Pixel values from 0-255\n",
    "    fake = np.clip((fake+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)\n",
    "    #Repeat for real image\n",
    "    real = real.transpose((0, 2, 3, 1))\n",
    "    real = np.clip((real+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)\n",
    "\n",
    "    #Create buffer array that will hold all the images in the batch\n",
    "    #Fill the buffer so to arrange all images in the batch onto the buffer array\n",
    "    n = np.ceil(np.sqrt(fake.shape[0]))\n",
    "    fbuff = np.zeros((int(n*fake.shape[1]), int(n*fake.shape[2]), int(fake.shape[3])), dtype=np.uint8)\n",
    "    for i, img in enumerate(fake):\n",
    "        fill_buf(fbuff, i, img, fake.shape[1:3])\n",
    "    rbuff = np.zeros((int(n*real.shape[1]), int(n*real.shape[2]), int(real.shape[3])), dtype=np.uint8)\n",
    "    for i, img in enumerate(real):\n",
    "        fill_buf(rbuff, i, img, real.shape[1:3])\n",
    "\n",
    "    #Create a matplotlib figure with two subplots: one for the real and the other for the fake\n",
    "    #fill each plot with the buffer array, which creates the image\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    ax1.imshow(fbuff)\n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2.imshow(rbuff)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============train===============\n",
    "print('Training...')\n",
    "for epoch in range(1):\n",
    "    image_iter.reset()\n",
    "    for i, batch in enumerate(image_iter):\n",
    "        #Get a batch of random numbers to generate an image from the generator\n",
    "        rbatch = rand_iter.next()\n",
    "        #Forward pass on training batch\n",
    "        generator.forward(rbatch, is_train=True)\n",
    "        #Output of training batch is the 64x64x3 image\n",
    "        outG = generator.get_outputs()\n",
    "\n",
    "        #Pass the generated (fake) image through the discriminator, and save the gradient\n",
    "        #Label (for logistic regression) is an array of 0's since this image is fake\n",
    "        label = mx.nd.zeros((batch_size,), ctx=ctx)\n",
    "        #Forward pass on the output of the discriminator network\n",
    "        discriminator.forward(mx.io.DataBatch(outG, [label]), is_train=True)\n",
    "        #Do the backward pass and save the gradient\n",
    "        discriminator.backward()\n",
    "        gradD = [[grad.copyto(grad.context) for grad in grads] for grads in discriminator._exec_group.grad_arrays]\n",
    "\n",
    "        #Pass a batch of real images from MNIST through the discriminator\n",
    "        #Set the label to be an array of 1's because these are the real images\n",
    "        label[:] = 1\n",
    "        batch.label = [label]\n",
    "        #Forward pass on a batch of MNIST images\n",
    "        discriminator.forward(batch, is_train=True)\n",
    "        #Do the backward pass and add the saved gradient from the fake images to the gradient\n",
    "        #generated by this backwards pass on the real images\n",
    "        discriminator.backward()\n",
    "        for gradsr, gradsf in zip(discriminator._exec_group.grad_arrays, gradD):\n",
    "            for gradr, gradf in zip(gradsr, gradsf):\n",
    "                gradr += gradf\n",
    "        #Update gradient on the discriminator\n",
    "        discriminator.update()\n",
    "\n",
    "        #Now that we've updated the discriminator, let's update the generator\n",
    "        #First do a forward pass and backwards pass on the newly updated discriminator\n",
    "        #With the current batch\n",
    "        discriminator.forward(mx.io.DataBatch(outG, [label]), is_train=True)\n",
    "        discriminator.backward()\n",
    "        #Get the input gradient from the backwards pass on the discriminator,\n",
    "        #and use it to do the backwards pass on the generator\n",
    "        diffD = discriminator.get_input_grads()\n",
    "        generator.backward(diffD)\n",
    "        #Update the gradients on the generator\n",
    "        generator.update()\n",
    "\n",
    "        #Increment to the next batch, printing every 50 batches\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            print('epoch:', epoch, 'iter:', i)\n",
    "            print\n",
    "            print(\"   From generator:        From MNIST:\")\n",
    "\n",
    "            print(\"jes\")\n",
    "            # visualize(outG[0].asnumpy(), batch.data[0].asnumpy())\n",
    "            print(outG[0].asnumpy())\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "display_name": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
